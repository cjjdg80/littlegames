# scripts/crawler/game_detail_requests.py - ä½¿ç”¨requestsç›´æ¥è·å–æ¸¸æˆè¯¦æƒ…ï¼ˆæ— æµè§ˆå™¨ç‰ˆæœ¬ï¼‰

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from urllib.parse import urljoin, urlparse

def get_game_details_requests(url):
    """
    ä½¿ç”¨requests + BeautifulSoupè·å–æ¸¸æˆè¯¦æƒ…
    ç›¸æ¯”Seleniumç‰ˆæœ¬ï¼Œé€Ÿåº¦æå‡10-20å€
    """
    print(f"\nğŸš€ å¼€å§‹åˆ†ææ¸¸æˆ: {url}")
    
    # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # å‘é€HTTPè¯·æ±‚
        print("ğŸ“¡ å‘é€HTTPè¯·æ±‚...")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        print(f"âœ… é¡µé¢è·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ é¡µé¢å¤§å°: {len(response.content)} bytes")
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ¸¸æˆIDï¼ˆä»URLä¸­è·å–ï¼‰
        game_id = extract_game_id_from_url(url)
        
        # æ„å»ºæ¸¸æˆæ•°æ®ç»“æ„
        game_data = {
            "basic_info": {
                "id": game_id,
                "name": "",
                "url": url,
                "company": "æœªçŸ¥å¼€å‘å•†",
                "collected_at": datetime.now().isoformat()
            },
            "extraction_time": datetime.now().isoformat(),
            "url": url,
            "game_info": extract_game_info(soup),
            "genres": extract_genres(soup),
            "tags": extract_tags(soup),
            "thumbnails": extract_thumbnails(soup),
            "iframe_code": extract_iframe_code(soup),
            "description": extract_description(soup),
            "instructions": extract_instructions(soup)
        }
        
        # æ›´æ–°åŸºæœ¬ä¿¡æ¯ä¸­çš„æ¸¸æˆåç§°
        if game_data["game_info"].get("title"):
            game_data["basic_info"]["name"] = game_data["game_info"]["title"]
        
        # æ›´æ–°å‘å¸ƒå•†ä¿¡æ¯
        if game_data["game_info"].get("publisher"):
            game_data["basic_info"]["company"] = game_data["game_info"]["publisher"]
        
        print("âœ… æ•°æ®æå–å®Œæˆ")
        return game_data
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return None
    except Exception as e:
        print(f"âŒ æ•°æ®æå–å¤±è´¥: {str(e)}")
        return None

def extract_game_id_from_url(url):
    """ä»URLä¸­æå–æ¸¸æˆID"""
    try:
        # ä»URLè·¯å¾„ä¸­æå–æ¸¸æˆID
        path = urlparse(url).path
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„æ–œæ ï¼Œç„¶ååˆ†å‰²
        parts = path.strip('/').split('/')
        if len(parts) >= 2 and parts[0] == 'games':
            return parts[1]
        return "unknown-game"
    except:
        return "unknown-game"

def extract_game_info(soup):
    """æå–æ¸¸æˆåŸºæœ¬ä¿¡æ¯"""
    print("ğŸ“‹ æå–æ¸¸æˆåŸºæœ¬ä¿¡æ¯...")
    info = {}
    
    try:
        # æå–æ¸¸æˆæ ‡é¢˜
        title_elem = soup.find('h1')
        if title_elem:
            info['title'] = title_elem.get_text().strip()
            print(f"  ğŸ“ æ ‡é¢˜: {info['title']}")
        
        # æå–å‘å¸ƒè€…ä¿¡æ¯
        publisher_link = soup.find('a', href=lambda x: x and 'company=' in x)
        if publisher_link:
            info['publisher'] = publisher_link.get_text().strip()
            info['publisher_url'] = publisher_link.get('href')
            print(f"  ğŸ¢ å‘å¸ƒè€…: {info['publisher']}")
        
        # æå–ç§»åŠ¨ç«¯å…¼å®¹æ€§
        mobile_text = soup.find(string=re.compile(r'Mobile Web Compatible', re.I))
        if mobile_text:
            info['mobile_compatible'] = mobile_text.strip()
            print(f"  ğŸ“± ç§»åŠ¨ç«¯å…¼å®¹: {info['mobile_compatible']}")
        
        # æå–æ”¯æŒçš„è¯­è¨€
        languages = []
        lang_section = soup.find(string=re.compile(r'Languages', re.I))
        if lang_section:
            lang_parent = lang_section.find_parent()
            if lang_parent:
                lang_links = lang_parent.find_all('a')
                languages = [link.get_text().strip() for link in lang_links if link.get_text().strip()]
        info['languages'] = languages
        if languages:
            print(f"  ğŸŒ æ”¯æŒè¯­è¨€: {', '.join(languages)}")
        
        # æå–æ€§åˆ«æ ‡ç­¾
        gender_tags = []
        gender_section = soup.find(string=re.compile(r'Gender', re.I))
        if gender_section:
            gender_parent = gender_section.find_parent()
            if gender_parent:
                gender_links = gender_parent.find_all('a')
                gender_tags = [link.get_text().strip() for link in gender_links if link.get_text().strip()]
        info['gender_tags'] = gender_tags
        
        # æå–å¹´é¾„ç»„
        age_groups = []
        age_section = soup.find(string=re.compile(r'Age', re.I))
        if age_section:
            age_parent = age_section.find_parent()
            if age_parent:
                age_links = age_parent.find_all('a')
                age_groups = [link.get_text().strip() for link in age_links if link.get_text().strip()]
        info['age_groups'] = age_groups
        
    except Exception as e:
        print(f"  âš ï¸ æå–æ¸¸æˆä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    return info

def extract_genres(soup):
    """æå–æ¸¸æˆç±»å‹"""
    print("ğŸ® æå–æ¸¸æˆç±»å‹...")
    genres = []
    
    try:
        # æŸ¥æ‰¾åŒ…å«"Genres"çš„æ–‡æœ¬
        genres_section = soup.find(string=re.compile(r'Genres', re.I))
        if genres_section:
            genres_parent = genres_section.find_parent()
            if genres_parent:
                genre_links = genres_parent.find_all('a')
                genres = [link.get_text().strip() for link in genre_links if link.get_text().strip()]
        
        if genres:
            print(f"  ğŸ·ï¸ æ¸¸æˆç±»å‹: {', '.join(genres)}")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°æ¸¸æˆç±»å‹ä¿¡æ¯")
            
    except Exception as e:
        print(f"  âš ï¸ æå–æ¸¸æˆç±»å‹æ—¶å‡ºé”™: {str(e)}")
    
    return genres

def extract_tags(soup):
    """æå–æ¸¸æˆæ ‡ç­¾"""
    print("ğŸ·ï¸ æå–æ¸¸æˆæ ‡ç­¾...")
    tags = []
    
    try:
        # æŸ¥æ‰¾åŒ…å«"Tags"çš„æ–‡æœ¬
        tags_section = soup.find(string=re.compile(r'Tags', re.I))
        if tags_section:
            tags_parent = tags_section.find_parent()
            if tags_parent:
                tag_links = tags_parent.find_all('a')
                tags = [link.get_text().strip() for link in tag_links if link.get_text().strip()]
        
        if tags:
            print(f"  ğŸ·ï¸ æ¸¸æˆæ ‡ç­¾: {', '.join(tags)}")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°æ¸¸æˆæ ‡ç­¾ä¿¡æ¯")
            
    except Exception as e:
        print(f"  âš ï¸ æå–æ¸¸æˆæ ‡ç­¾æ—¶å‡ºé”™: {str(e)}")
    
    return tags

def extract_thumbnails(soup):
    """æå–ç¼©ç•¥å›¾"""
    print("ğŸ–¼ï¸ æå–ç¼©ç•¥å›¾...")
    thumbnails = []
    
    try:
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å…ƒç´ 
        img_elements = soup.find_all('img')
        
        for img in img_elements:
            src = img.get('src', '')
            if 'gamedistribution.com' in src and any(size in src for size in ['512x384', '512x512', '200x120', '1280x720', '1280x550']):
                # æå–å°ºå¯¸ä¿¡æ¯
                size_match = re.search(r'(\d+x\d+)', src)
                size = size_match.group(1) if size_match else "unknown"
                
                thumbnail = {
                    "url": src,
                    "size": size,
                    "alt": img.get('alt', src.split('/')[-1])
                }
                thumbnails.append(thumbnail)
        
        if thumbnails:
            print(f"  ğŸ–¼ï¸ æ‰¾åˆ° {len(thumbnails)} å¼ ç¼©ç•¥å›¾")
            for thumb in thumbnails:
                print(f"    - {thumb['size']}: {thumb['url']}")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°ç¼©ç•¥å›¾")
            
    except Exception as e:
        print(f"  âš ï¸ æå–ç¼©ç•¥å›¾æ—¶å‡ºé”™: {str(e)}")
    
    return thumbnails

def extract_iframe_code(soup):
    """æå–iframeä»£ç """
    print("ğŸ¯ æå–iframeä»£ç ...")
    iframe_data = {}
    
    try:
        # æŸ¥æ‰¾iframeå…ƒç´ 
        iframe = soup.find('iframe')
        if iframe:
            src = iframe.get('src', '')
            width = iframe.get('width', '960')
            height = iframe.get('height', '600')
            
            # æ„å»ºå®Œæ•´çš„iframeä»£ç 
            full_code = f'<iframe src="{src}" width="{width}" height="{height}" scrolling="none" frameborder="0"></iframe>'
            
            iframe_data = {
                "full_code": full_code,
                "src": src,
                "width": width,
                "height": height
            }
            
            print(f"  ğŸ¯ iframeæº: {src}")
            print(f"  ğŸ“ å°ºå¯¸: {width}x{height}")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°iframeå…ƒç´ ")
            
    except Exception as e:
        print(f"  âš ï¸ æå–iframeä»£ç æ—¶å‡ºé”™: {str(e)}")
    
    return iframe_data

def extract_description(soup):
    """æå–æ¸¸æˆæè¿°"""
    print("ğŸ“ æå–æ¸¸æˆæè¿°...")
    description = ""
    
    try:
        # æŸ¥æ‰¾æè¿°æ–‡æœ¬
        desc_elem = soup.find('meta', {'name': 'description'})
        if desc_elem:
            description = desc_elem.get('content', '').strip()
        
        # å¦‚æœmetaæè¿°ä¸ºç©ºï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–æè¿°å…ƒç´ 
        if not description:
            # æŸ¥æ‰¾åŒ…å«æ¸¸æˆæè¿°çš„æ®µè½
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 100:  # å‡è®¾æè¿°è‡³å°‘æœ‰100ä¸ªå­—ç¬¦
                    description = text
                    break
        
        if description:
            print(f"  ğŸ“ æè¿°é•¿åº¦: {len(description)} å­—ç¬¦")
            print(f"  ğŸ“ æè¿°é¢„è§ˆ: {description[:100]}...")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°æ¸¸æˆæè¿°")
            
    except Exception as e:
        print(f"  âš ï¸ æå–æ¸¸æˆæè¿°æ—¶å‡ºé”™: {str(e)}")
    
    return description

def extract_instructions(soup):
    """æå–æ“ä½œè¯´æ˜"""
    print("ğŸ® æå–æ“ä½œè¯´æ˜...")
    instructions = ""
    
    try:
        # æŸ¥æ‰¾åŒ…å«æ“ä½œè¯´æ˜çš„æ–‡æœ¬
        # é€šå¸¸åŒ…å«"PLAYER"ã€"Movement"ã€"Jump"ç­‰å…³é”®è¯
        text_elements = soup.find_all(string=re.compile(r'PLAYER|Movement|Jump|SPACE|ARROW', re.I))
        
        for text in text_elements:
            if len(text.strip()) > 50:  # å‡è®¾æ“ä½œè¯´æ˜è‡³å°‘æœ‰50ä¸ªå­—ç¬¦
                instructions = text.strip()
                break
        
        if instructions:
            print(f"  ğŸ® æ“ä½œè¯´æ˜é•¿åº¦: {len(instructions)} å­—ç¬¦")
            print(f"  ğŸ® æ“ä½œè¯´æ˜é¢„è§ˆ: {instructions[:100]}...")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ°æ“ä½œè¯´æ˜")
            
    except Exception as e:
        print(f"  âš ï¸ æå–æ“ä½œè¯´æ˜æ—¶å‡ºé”™: {str(e)}")
    
    return instructions

def test_single_game():
    """æµ‹è¯•å•ä¸ªæ¸¸æˆçš„æ•°æ®æå–"""
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¸¸æˆè¿›è¡Œæµ‹è¯•
    test_url = "https://gamedistribution.com/games/obby-survive-parkour/"
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•requestsç‰ˆæœ¬çš„æ•°æ®æå–...")
    print(f"ğŸ¯ æµ‹è¯•URL: {test_url}")
    
    start_time = time.time()
    
    # æå–æ¸¸æˆæ•°æ®
    game_data = get_game_details_requests(test_url)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    if game_data:
        print(f"\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        output_file = "scripts/output/game_detail_requests_test.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(game_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºæå–åˆ°çš„æ•°æ®æ‘˜è¦
        print("\nğŸ“Š æ•°æ®æå–æ‘˜è¦:")
        print(f"  ğŸ® æ¸¸æˆåç§°: {game_data.get('game_info', {}).get('title', 'N/A')}")
        print(f"  ğŸ¢ å‘å¸ƒè€…: {game_data.get('game_info', {}).get('publisher', 'N/A')}")
        print(f"  ğŸ¯ æ¸¸æˆç±»å‹: {', '.join(game_data.get('genres', []))}")
        print(f"  ğŸ·ï¸ æ ‡ç­¾æ•°é‡: {len(game_data.get('tags', []))}")
        print(f"  ğŸ–¼ï¸ ç¼©ç•¥å›¾æ•°é‡: {len(game_data.get('thumbnails', []))}")
        print(f"  ğŸ¯ iframe: {'âœ…' if game_data.get('iframe_code', {}).get('src') else 'âŒ'}")
        print(f"  ğŸ“ æè¿°: {'âœ…' if game_data.get('description') else 'âŒ'}")
        print(f"  ğŸ® æ“ä½œè¯´æ˜: {'âœ…' if game_data.get('instructions') else 'âŒ'}")
        
        return True
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        return False

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_single_game()
    
    if success:
        print("\nğŸ‰ requestsç‰ˆæœ¬æµ‹è¯•æˆåŠŸï¼")
        print("ğŸ’¡ ç›¸æ¯”Seleniumç‰ˆæœ¬çš„ä¼˜åŠ¿:")
        print("  âš¡ é€Ÿåº¦æå‡: 10-20å€")
        print("  ğŸ’¾ å†…å­˜å ç”¨: é™ä½90%")
        print("  ğŸ”§ ç»´æŠ¤æˆæœ¬: æ›´ä½")
        print("  ğŸš€ æ‰¹é‡å¤„ç†: æ›´é€‚åˆ")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("  1. éªŒè¯æå–æ•°æ®çš„å®Œæ•´æ€§")
        print("  2. æµ‹è¯•æ›´å¤šæ¸¸æˆURL")
        print("  3. åˆ›å»ºæ‰¹é‡å¤„ç†ç‰ˆæœ¬")
    else:
        print("\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦:")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  2. åˆ†æé¡µé¢ç»“æ„å˜åŒ–")
        print("  3. è°ƒæ•´æå–é€»è¾‘")
        print("  4. è€ƒè™‘ä½¿ç”¨Seleniumä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")